{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you submit this notebook, make sure everything runs as expected in the local test cases. \n",
    "Please, paste the solution to the designed cell and do not change anything else.\n",
    "\n",
    "Also, please, leave your first and last names below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "FirstName = \"Andrew\"\n",
    "LastName = \"Kussev\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "54c6398a05b33742cb4853125e3f7359",
     "grade": false,
     "grade_id": "cell-ac8fc52d8a39ccb4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    \"\"\"\n",
    "    Basically, you can think of a module as of a something (black box) \n",
    "    which can process `input` data and produce `ouput` data.\n",
    "    This is like applying a function which is called `forward`: \n",
    "        \n",
    "        output = module.forward(input)\n",
    "    \n",
    "    The module should be able to perform a backward pass: to differentiate the `forward` function. \n",
    "    More, it should be able to differentiate it if is a part of chain (chain rule).\n",
    "    The latter implies there is a gradient from previous step of a chain rule. \n",
    "    \n",
    "        gradInput = module.backward(input, gradOutput)\n",
    "    \"\"\"\n",
    "    def __init__ (self):\n",
    "        self.output = None\n",
    "        self.gradInput = None\n",
    "        self.training = True\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Takes an input object, and computes the corresponding output of the module.\n",
    "        \"\"\"\n",
    "        return self.updateOutput(input)\n",
    "\n",
    "    def backward(self,input, gradOutput):\n",
    "        \"\"\"\n",
    "        Performs a backpropagation step through the module, with respect to the given input.\n",
    "        \n",
    "        This includes \n",
    "         - computing a gradient w.r.t. `input` (is needed for further backprop),\n",
    "         - computing a gradient w.r.t. parameters (to update parameters while optimizing).\n",
    "        \"\"\"\n",
    "        self.updateGradInput(input, gradOutput)\n",
    "        self.accGradParameters(input, gradOutput)\n",
    "        return self.gradInput\n",
    "    \n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        \"\"\"\n",
    "        Computes the output using the current parameter set of the class and input.\n",
    "        This function returns the result which is stored in the `output` field.\n",
    "        \n",
    "        Make sure to both store the data in `output` field and return it. \n",
    "        \"\"\"\n",
    "        \n",
    "        # The easiest case:\n",
    "            \n",
    "        # self.output = input \n",
    "        # return self.output\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        \"\"\"\n",
    "        Computing the gradient of the module with respect to its own input. \n",
    "        This is returned in `gradInput`. Also, the `gradInput` state variable is updated accordingly.\n",
    "        \n",
    "        The shape of `gradInput` is always the same as the shape of `input`.\n",
    "        \n",
    "        Make sure to both store the gradients in `gradInput` field and return it.\n",
    "        \"\"\"\n",
    "        \n",
    "        # The easiest case:\n",
    "        \n",
    "        # self.gradInput = gradOutput \n",
    "        # return self.gradInput\n",
    "        \n",
    "        pass   \n",
    "    \n",
    "    def accGradParameters(self, input, gradOutput):\n",
    "        \"\"\"\n",
    "        Computing the gradient of the module with respect to its own parameters.\n",
    "        No need to override if module has no parameters (e.g. ReLU).\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def zeroGradParameters(self): \n",
    "        \"\"\"\n",
    "        Zeroes `gradParams` variable if the module has params.\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    def getParameters(self):\n",
    "        \"\"\"\n",
    "        Returns a list with its parameters. \n",
    "        If the module does not have parameters return empty list. \n",
    "        \"\"\"\n",
    "        return []\n",
    "        \n",
    "    def getGradParameters(self):\n",
    "        \"\"\"\n",
    "        Returns a list with gradients with respect to its parameters. \n",
    "        If the module does not have parameters return empty list. \n",
    "        \"\"\"\n",
    "        return []\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Sets training mode for the module.\n",
    "        Training and testing behaviour differs for Dropout, BatchNorm.\n",
    "        \"\"\"\n",
    "        self.training = True\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Sets evaluation mode for the module.\n",
    "        Training and testing behaviour differs for Dropout, BatchNorm.\n",
    "        \"\"\"\n",
    "        self.training = False\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Pretty printing. Should be overrided in every module if you want \n",
    "        to have readable description. \n",
    "        \"\"\"\n",
    "        return \"Module\"\n",
    "\n",
    "\n",
    "class Sequential(Module):\n",
    "    \"\"\"\n",
    "         This class implements a container, which processes `input` data sequentially. \n",
    "         \n",
    "         `input` is processed by each module (layer) in self.modules consecutively.\n",
    "         The resulting array is called `output`. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__ (self):\n",
    "        super(Sequential, self).__init__()\n",
    "        self.modules = []\n",
    "   \n",
    "    def add(self, module):\n",
    "        \"\"\"\n",
    "        Adds a module to the container.\n",
    "        \"\"\"\n",
    "        self.modules.append(module)\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        \"\"\"\n",
    "        Basic workflow of FORWARD PASS:\n",
    "        \n",
    "            y_0    = module[0].forward(input)\n",
    "            y_1    = module[1].forward(y_0)\n",
    "            ...\n",
    "            output = module[n-1].forward(y_{n-2})   \n",
    "            \n",
    "            \n",
    "        Just write a little loop. \n",
    "        \"\"\"\n",
    "        self.output = input\n",
    "        for module in self.modules:\n",
    "            self.output = module.forward(self.output)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, input, gradOutput):\n",
    "        \"\"\"\n",
    "        Workflow of BACKWARD PASS:\n",
    "            \n",
    "            g_{n-1} = module[n-1].backward(y_{n-2}, gradOutput)\n",
    "            g_{n-2} = module[n-2].backward(y_{n-3}, g_{n-1})\n",
    "            ...\n",
    "            g_1 = module[1].backward(y_0, g_2)   \n",
    "            gradInput = module[0].backward(input, g_1)   \n",
    "             \n",
    "             \n",
    "        !!!\n",
    "                \n",
    "        To ech module you need to provide the input, module saw while forward pass, \n",
    "        it is used while computing gradients. \n",
    "        Make sure that the input for `i-th` layer the output of `module[i]` (just the same input as in forward pass) \n",
    "        and NOT `input` to this Sequential module. \n",
    "        \n",
    "        !!!\n",
    "        \n",
    "        \"\"\"\n",
    "        n = len(self.modules)\n",
    "        self.gradInput = gradOutput\n",
    "        for i in range(n - 1, 0, -1):\n",
    "            module = self.modules[i]\n",
    "            current_input = self.modules[i - 1].output\n",
    "            self.gradInput = module.backward(current_input, self.gradInput)\n",
    "        self.gradInput = self.modules[0].backward(input, self.gradInput)\n",
    "        return self.gradInput\n",
    "      \n",
    "\n",
    "    def zeroGradParameters(self): \n",
    "        for module in self.modules:\n",
    "            module.zeroGradParameters()\n",
    "    \n",
    "    def getParameters(self):\n",
    "        \"\"\"\n",
    "        Should gather all parameters in a list.\n",
    "        \"\"\"\n",
    "        return [x.getParameters() for x in self.modules]\n",
    "    \n",
    "    def getGradParameters(self):\n",
    "        \"\"\"\n",
    "        Should gather all gradients w.r.t parameters in a list.\n",
    "        \"\"\"\n",
    "        return [x.getGradParameters() for x in self.modules]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        string = \"\".join([str(x) + '\\n' for x in self.modules])\n",
    "        return string\n",
    "    \n",
    "    def __getitem__(self,x):\n",
    "        return self.modules.__getitem__(x)\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Propagates training parameter through all modules\n",
    "        \"\"\"\n",
    "        self.training = True\n",
    "        for module in self.modules:\n",
    "            module.train()\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Propagates training parameter through all modules\n",
    "        \"\"\"\n",
    "        self.training = False\n",
    "        for module in self.modules:\n",
    "            module.evaluate()\n",
    "\n",
    "class Linear(Module):\n",
    "    \"\"\"\n",
    "    A module which applies a linear transformation \n",
    "    A common name is fully-connected layer, InnerProductLayer in caffe. \n",
    "    \n",
    "    The module should work with 2D input of shape (n_samples, n_feature).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super(Linear, self).__init__()\n",
    "       \n",
    "        # This is a nice initialization\n",
    "        stdv = 1. / np.sqrt(n_in)\n",
    "        self.W = np.random.uniform(-stdv, stdv, size = (n_out, n_in))\n",
    "        self.b = np.random.uniform(-stdv, stdv, size = n_out)\n",
    "        \n",
    "        self.gradW = np.zeros_like(self.W)\n",
    "        self.gradb = np.zeros_like(self.b)\n",
    "        \n",
    "    def updateOutput(self, input):\n",
    "        self.output = input @ self.W.T + self.b\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = gradOutput @ self.W\n",
    "        return self.gradInput\n",
    "    \n",
    "    def accGradParameters(self, input, gradOutput):\n",
    "        self.gradW = gradOutput.T @ input\n",
    "        self.gradb = gradOutput.sum(axis=0)\n",
    "    \n",
    "    def zeroGradParameters(self):\n",
    "        self.gradW.fill(0)\n",
    "        self.gradb.fill(0)\n",
    "        \n",
    "    def getParameters(self):\n",
    "        return [self.W, self.b]\n",
    "    \n",
    "    def getGradParameters(self):\n",
    "        return [self.gradW, self.gradb]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        s = self.W.shape\n",
    "        q = 'Linear %d -> %d' %(s[1],s[0])\n",
    "        return q\n",
    "\n",
    "class SoftMax(Module):\n",
    "    def __init__(self):\n",
    "         super(SoftMax, self).__init__()\n",
    "    \n",
    "    def updateOutput(self, input):\n",
    "        # start with normalization for numerical stability\n",
    "        self.output = np.subtract(input, input.max(axis=1, keepdims=True))\n",
    "        \n",
    "        exp = np.exp(self.output)\n",
    "        self.output = exp / exp.sum(axis=1, keepdims=True)\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        batch_size = input.shape[0]\n",
    "        n_feats = input.shape[1]\n",
    "        exp = np.exp(input)\n",
    "        exp_sum = exp.sum(axis=1, keepdims=True)\n",
    "        \n",
    "        self.gradInput = (exp * gradOutput / exp_sum) @ np.eye(n_feats)\n",
    "        \n",
    "        self.gradInput -= exp * (exp * gradOutput / exp_sum ** 2).sum(axis=1, keepdims=True)\n",
    "        \n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"SoftMax\"\n",
    "\n",
    "class LogSoftMax(Module):\n",
    "    def __init__(self):\n",
    "        super(LogSoftMax, self).__init__()\n",
    "    \n",
    "    def updateOutput(self, input):\n",
    "        # start with normalization for numerical stability\n",
    "        self.output = np.subtract(input, input.max(axis=1, keepdims=True))\n",
    "        \n",
    "        exp = np.exp(self.output)\n",
    "        self.output = np.log(exp / exp.sum(axis=1, keepdims=True))\n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        n_feats = input.shape[1]\n",
    "        exp = np.exp(input)\n",
    "        exp_sum = exp.sum(axis=1, keepdims=True)\n",
    "        \n",
    "        self.gradInput = gradOutput @ np.eye(n_feats)\n",
    "        \n",
    "        self.gradInput -= exp * (gradOutput).sum(axis=1, keepdims=True) / exp_sum\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"LogSoftMax\"\n",
    "\n",
    "class BatchNormalization(Module):\n",
    "    EPS = 1\n",
    "    def __init__(self, alpha = 0.):\n",
    "        super(BatchNormalization, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.moving_mean = 0\n",
    "        self.moving_variance = 1\n",
    "        self.batch_mean = None\n",
    "        self.std = None\n",
    "        \n",
    "    def updateOutput(self, input):\n",
    "        if self.training:\n",
    "            self.batch_mean = input.mean(axis=0)\n",
    "            batch_variance = input.var(axis=0)\n",
    "            self.std = np.sqrt(self.EPS + batch_variance)\n",
    "            \n",
    "            self.output = (input - self.batch_mean) / self.std\n",
    "            \n",
    "            self.moving_mean = self.moving_mean * self.alpha + self.batch_mean * (1 - self.alpha)\n",
    "            self.moving_variance = self.moving_variance * self.alpha + batch_variance * (1 - self.alpha)\n",
    "        else:\n",
    "            self.output = (input - self.moving_mean) / np.sqrt(self.EPS + self.moving_variance)\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        if self.training:\n",
    "            n_feats = input.shape[0]\n",
    "            \n",
    "            self.gradInput = (np.eye(n_feats) * n_feats - 1) @ gradOutput\n",
    "            self.gradInput -= self.output * (self.output * gradOutput).sum(axis=0)\n",
    "            self.gradInput /= n_feats * self.std\n",
    "        else:\n",
    "            self.gradInput = gradOutput / np.sqrt(self.EPS + self.moving_variance)\n",
    "        \n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"BatchNormalization\"\n",
    "\n",
    "class ChannelwiseScaling(Module):\n",
    "    \"\"\"\n",
    "       Implements linear transform of input y = \\gamma * x + \\beta\n",
    "       where \\gamma, \\beta - learnable vectors of length x.shape[-1]\n",
    "    \"\"\"\n",
    "    def __init__(self, n_out):\n",
    "        super(ChannelwiseScaling, self).__init__()\n",
    "\n",
    "        stdv = 1./np.sqrt(n_out)\n",
    "        self.gamma = np.random.uniform(-stdv, stdv, size=n_out)\n",
    "        self.beta = np.random.uniform(-stdv, stdv, size=n_out)\n",
    "        \n",
    "        self.gradGamma = np.zeros_like(self.gamma)\n",
    "        self.gradBeta = np.zeros_like(self.beta)\n",
    "\n",
    "    def updateOutput(self, input):\n",
    "        self.output = input * self.gamma + self.beta\n",
    "        return self.output\n",
    "        \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = gradOutput * self.gamma\n",
    "        return self.gradInput\n",
    "    \n",
    "    def accGradParameters(self, input, gradOutput):\n",
    "        self.gradBeta = np.sum(gradOutput, axis=0)\n",
    "        self.gradGamma = np.sum(gradOutput*input, axis=0)\n",
    "    \n",
    "    def zeroGradParameters(self):\n",
    "        self.gradGamma.fill(0)\n",
    "        self.gradBeta.fill(0)\n",
    "        \n",
    "    def getParameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "    \n",
    "    def getGradParameters(self):\n",
    "        return [self.gradGamma, self.gradBeta]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"ChannelwiseScaling\"\n",
    "\n",
    "class Dropout(Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super(Dropout, self).__init__()\n",
    "        \n",
    "        self.p = p\n",
    "        self.mask = None\n",
    "        \n",
    "    def updateOutput(self, input):\n",
    "        if not self.training:\n",
    "            self.output = input\n",
    "        else:\n",
    "            self.mask = np.random.binomial(1, 1 - self.p, input.shape)\n",
    "            self.output = self.mask * input / (1 - self.p)\n",
    "        return  self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        if not self.training:\n",
    "            self.gradInput = gradOutput\n",
    "        else:\n",
    "            self.gradInput = gradOutput * self.mask / (1 - self.p)\n",
    "        return self.gradInput\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"Dropout\"\n",
    "\n",
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "         super(ReLU, self).__init__()\n",
    "    \n",
    "    def updateOutput(self, input):\n",
    "        self.output = np.maximum(input, 0)\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = np.multiply(gradOutput , input > 0)\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"ReLU\"\n",
    "\n",
    "class LeakyReLU(Module):\n",
    "    def __init__(self, slope = 0.03):\n",
    "        super(LeakyReLU, self).__init__()\n",
    "            \n",
    "        self.slope = slope\n",
    "        \n",
    "    def updateOutput(self, input):\n",
    "        self.output = np.maximum(input, 0) + self.slope * np.minimum(input, 0)\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = gradOutput * ((1 - self.slope) * (input > 0) + self.slope)\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"LeakyReLU\"\n",
    "\n",
    "class ELU(Module):\n",
    "    def __init__(self, alpha = 1.0):\n",
    "        super(ELU, self).__init__()\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def updateOutput(self, input):\n",
    "        self.output = np.maximum(input, 0) + self.alpha * (np.exp(np.minimum(input, 0)) - 1)\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = gradOutput * ((1 - self.alpha) * (input > 0) + self.alpha * np.exp(np.minimum(input, 0)))\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"ELU\"\n",
    "\n",
    "class SoftPlus(Module):\n",
    "    def __init__(self):\n",
    "        super(SoftPlus, self).__init__()\n",
    "    \n",
    "    def updateOutput(self, input):\n",
    "        self.output = np.log(1 + np.exp(input))\n",
    "        return  self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = gradOutput * (1 - 1 / (1 + np.exp(input)))\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"SoftPlus\"\n",
    "\n",
    "class Criterion(object):\n",
    "    def __init__ (self):\n",
    "        self.output = None\n",
    "        self.gradInput = None\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        \"\"\"\n",
    "            Given an input and a target, compute the loss function \n",
    "            associated to the criterion and return the result.\n",
    "            \n",
    "            For consistency this function should not be overrided,\n",
    "            all the code goes in `updateOutput`.\n",
    "        \"\"\"\n",
    "        return self.updateOutput(input, target)\n",
    "\n",
    "    def backward(self, input, target):\n",
    "        \"\"\"\n",
    "            Given an input and a target, compute the gradients of the loss function\n",
    "            associated to the criterion and return the result. \n",
    "\n",
    "            For consistency this function should not be overrided,\n",
    "            all the code goes in `updateGradInput`.\n",
    "        \"\"\"\n",
    "        return self.updateGradInput(input, target)\n",
    "    \n",
    "    def updateOutput(self, input, target):\n",
    "        \"\"\"\n",
    "        Function to override.\n",
    "        \"\"\"\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, target):\n",
    "        \"\"\"\n",
    "        Function to override.\n",
    "        \"\"\"\n",
    "        return self.gradInput   \n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Pretty printing. Should be overrided in every module if you want \n",
    "        to have readable description. \n",
    "        \"\"\"\n",
    "        return \"Criterion\"\n",
    "\n",
    "class MSECriterion(Criterion):\n",
    "    def __init__(self):\n",
    "        super(MSECriterion, self).__init__()\n",
    "        \n",
    "    def updateOutput(self, input, target):   \n",
    "        self.output = np.sum(np.power(input - target,2)) / input.shape[0]\n",
    "        return self.output \n",
    " \n",
    "    def updateGradInput(self, input, target):\n",
    "        self.gradInput  = (input - target) * 2 / input.shape[0]\n",
    "        return self.gradInput\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"MSECriterion\"\n",
    "\n",
    "class ClassNLLCriterionUnstable(Criterion):\n",
    "    EPS = 1e-15\n",
    "    def __init__(self):\n",
    "        a = super(ClassNLLCriterionUnstable, self)\n",
    "        super(ClassNLLCriterionUnstable, self).__init__()\n",
    "        \n",
    "    def updateOutput(self, input, target): \n",
    "        \n",
    "        # Use this trick to avoid numerical errors\n",
    "        input_clamp = np.clip(input, self.EPS, 1 - self.EPS)\n",
    "        \n",
    "        self.output = - (np.log(input_clamp) * target).sum() / input.shape[0]\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, target):\n",
    "        \n",
    "        # Use this trick to avoid numerical errors\n",
    "        input_clamp = np.clip(input, self.EPS, 1 - self.EPS)\n",
    "        \n",
    "        self.gradInput = -target / input_clamp / input.shape[0]\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"ClassNLLCriterionUnstable\"\n",
    "\n",
    "class ClassNLLCriterion(Criterion):\n",
    "    def __init__(self):\n",
    "        a = super(ClassNLLCriterion, self)\n",
    "        super(ClassNLLCriterion, self).__init__()\n",
    "        \n",
    "    def updateOutput(self, input, target): \n",
    "        self.output = - (input * target).sum() / input.shape[0]\n",
    "        return self.output\n",
    "\n",
    "    def updateGradInput(self, input, target):\n",
    "        self.gradInput = -target / input.shape[0]\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"ClassNLLCriterion\"\n",
    "\n",
    "def sgd_momentum(variables, gradients, config, state):  \n",
    "    # 'variables' and 'gradients' have complex structure, accumulated_grads will be stored in a simpler one\n",
    "    state.setdefault('accumulated_grads', {})\n",
    "    \n",
    "    var_index = 0 \n",
    "    for current_layer_vars, current_layer_grads in zip(variables, gradients): \n",
    "        for current_var, current_grad in zip(current_layer_vars, current_layer_grads):\n",
    "            \n",
    "            old_grad = state['accumulated_grads'].setdefault(var_index, np.zeros_like(current_grad))\n",
    "            \n",
    "            np.add(config['momentum'] * old_grad, config['learning_rate'] * current_grad, out=old_grad)\n",
    "            \n",
    "            current_var -= old_grad\n",
    "            var_index += 1     \n",
    "\n",
    "def adam_optimizer(variables, gradients, config, state):  \n",
    "    # 'variables' and 'gradients' have complex structure, accumulated_grads will be stored in a simpler one\n",
    "    state.setdefault('m', {})  # first moment vars\n",
    "    state.setdefault('v', {})  # second moment vars\n",
    "    state.setdefault('t', 0)   # timestamp\n",
    "    state['t'] += 1\n",
    "    for k in ['learning_rate', 'beta1', 'beta2', 'epsilon']:\n",
    "        assert k in config, config.keys()\n",
    "    \n",
    "    var_index = 0 \n",
    "    lr_t = config['learning_rate'] * np.sqrt(1 - config['beta2'] ** state['t']) / (1 - config['beta1'] ** state['t'])\n",
    "    for current_layer_vars, current_layer_grads in zip(variables, gradients): \n",
    "        for current_var, current_grad in zip(current_layer_vars, current_layer_grads):\n",
    "            var_first_moment = state['m'].setdefault(var_index, np.zeros_like(current_grad))\n",
    "            var_second_moment = state['v'].setdefault(var_index, np.zeros_like(current_grad))\n",
    "            \n",
    "            # <YOUR CODE> #######################################\n",
    "            # update `current_var_first_moment`, `var_second_moment` and `current_var` values\n",
    "            beta1 = config['beta1']\n",
    "            beta2 = config['beta2']\n",
    "            np.add(beta1 * var_first_moment, (1 - beta1) * current_grad, out=var_first_moment)\n",
    "            np.add(beta2 * var_second_moment, (1 - beta2) * current_grad ** 2, out=var_second_moment)\n",
    "            current_var -= lr_t * var_first_moment / (config['epsilon'] + np.sqrt(var_second_moment))\n",
    "            \n",
    "            # small checks that you've updated the state; use np.add for rewriting np.arrays values\n",
    "            assert var_first_moment is state['m'].get(var_index)\n",
    "            assert var_second_moment is state['v'].get(var_index)\n",
    "            var_index += 1\n",
    "\n",
    "import scipy as sp\n",
    "import scipy.signal\n",
    "import skimage\n",
    "\n",
    "class Conv2d(Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size):\n",
    "        super(Conv2d, self).__init__()\n",
    "        assert kernel_size % 2 == 1, kernel_size\n",
    "       \n",
    "        stdv = 1./np.sqrt(in_channels)\n",
    "        self.W = np.random.uniform(-stdv, stdv, size = (out_channels, in_channels, kernel_size, kernel_size))\n",
    "        self.b = np.random.uniform(-stdv, stdv, size=(out_channels,))\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        self.gradW = np.zeros_like(self.W)\n",
    "        self.gradb = np.zeros_like(self.b)\n",
    "        \n",
    "    def updateOutput(self, input):\n",
    "        pad_size = self.kernel_size // 2\n",
    "        batch_size = input.shape[0]\n",
    "        h = input.shape[2]\n",
    "        w = input.shape[3]\n",
    "        # YOUR CODE ##############################\n",
    "        self.output = np.zeros((batch_size, self.out_channels, h, w))\n",
    "        for i in range(batch_size):\n",
    "            for j in range(self.out_channels):\n",
    "                # 1. zero-pad the input array\n",
    "                picture = np.pad(input[i], pad_width=pad_size)[pad_size:-pad_size]\n",
    "                \n",
    "                # 2. compute convolution using scipy.signal.correlate(... , mode='valid')\n",
    "                self.output[i][j] = scipy.signal.correlate(picture, self.W[j], mode='valid')\n",
    "                \n",
    "                # 3. add bias value\n",
    "                self.output[i][j] += self.b[j]\n",
    "                \n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        pad_size = self.kernel_size // 2\n",
    "        # YOUR CODE ##############################\n",
    "        \n",
    "        self.gradInput = np.zeros_like(input)\n",
    "        batch_size = input.shape[0]\n",
    "        for i in range(batch_size):\n",
    "            for j in range(self.in_channels):\n",
    "                # 1. zero-pad the gradOutput\n",
    "                padded_grad = np.pad(gradOutput[i], pad_width=pad_size)[pad_size:-pad_size]\n",
    "                \n",
    "                # 2. compute 'self.gradInput' value using scipy.signal.correlate(... , mode='valid')\n",
    "                self.gradInput[i][j] = scipy.signal.correlate(padded_grad, np.flip(self.W[:, j], axis=(1, 2)), mode='valid')\n",
    "        \n",
    "        return self.gradInput\n",
    "    \n",
    "    def accGradParameters(self, input, gradOutput):\n",
    "        pad_size = self.kernel_size // 2\n",
    "        # YOUR CODE #############\n",
    "        batch_size = input.shape[0]\n",
    "        self.gradW = np.zeros_like(self.W)\n",
    "        for i in range(batch_size):\n",
    "            for j in range(self.out_channels):\n",
    "                # 1. zero-pad the input\n",
    "                picture = np.pad(input[i], pad_width=pad_size)[pad_size:-pad_size]\n",
    "\n",
    "                # 2. compute 'self.gradW' using scipy.signal.correlate(... , mode='valid')\n",
    "                \n",
    "                self.gradW[j] += scipy.signal.correlate(picture, gradOutput[i][j:j+1], mode='valid')\n",
    "        \n",
    "        # 3. compute 'self.gradb' - formulas like in Linear of ChannelwiseScaling layers\n",
    "        self.gradb = np.sum(gradOutput, axis=(0, 2, 3))\n",
    "        pass\n",
    "    \n",
    "    def zeroGradParameters(self):\n",
    "        self.gradW.fill(0)\n",
    "        self.gradb.fill(0)\n",
    "        \n",
    "    def getParameters(self):\n",
    "        return [self.W, self.b]\n",
    "    \n",
    "    def getGradParameters(self):\n",
    "        return [self.gradW, self.gradb]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        s = self.W.shape\n",
    "        q = 'Conv2d %d -> %d' %(s[1],s[0])\n",
    "        return q\n",
    "\n",
    "class MaxPool2d(Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super(MaxPool2d, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.gradInput = None\n",
    "                    \n",
    "    def updateOutput(self, input):\n",
    "        input_h, input_w = input.shape[-2:]\n",
    "        # your may remove these asserts and implement MaxPool2d with padding\n",
    "        assert input_h % self.kernel_size == 0  \n",
    "        assert input_w % self.kernel_size == 0\n",
    "        \n",
    "        batch_size = input.shape[0]\n",
    "        in_channels = input.shape[1]\n",
    "        input = input.reshape(batch_size, in_channels,\n",
    "                              input_h // self.kernel_size, self.kernel_size,\n",
    "                              input_w // self.kernel_size, self.kernel_size)\n",
    "        input = np.swapaxes(input, 3, 4)\n",
    "        input = input.reshape(batch_size, in_channels,\n",
    "                              input_h // self.kernel_size, input_w // self.kernel_size,\n",
    "                              self.kernel_size ** 2)\n",
    "        self.output = np.max(input, axis=-1)\n",
    "        self.max_indices = np.argmax(input, axis=-1)\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        batch_size = input.shape[0]\n",
    "        in_channels = input.shape[1]\n",
    "        input_h, input_w = input.shape[-2:]\n",
    "        self.gradInput = np.zeros((batch_size, in_channels,\n",
    "                                       input_h // self.kernel_size, input_w // self.kernel_size,\n",
    "                                       self.kernel_size ** 2))\n",
    "        for i in range(batch_size):\n",
    "            for j in range(in_channels):\n",
    "                for k in range(input_h // self.kernel_size):\n",
    "                    for l in range(input_w // self.kernel_size):\n",
    "                        self.gradInput[i][j][k][l][self.max_indices[i][j][k][l]] = gradOutput[i][j][k][l]\n",
    "        self.gradInput = self.gradInput.reshape(batch_size, in_channels,\n",
    "                              input_h // self.kernel_size, input_w // self.kernel_size,\n",
    "                              self.kernel_size, self.kernel_size)\n",
    "        self.gradInput = np.swapaxes(self.gradInput, 3, 4)\n",
    "        self.gradInput = self.gradInput.reshape((batch_size, in_channels, input_h, input_w))\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        q = 'MaxPool2d, kern %d, stride %d' %(self.kernel_size, self.kernel_size)\n",
    "        return q\n",
    "\n",
    "class Flatten(Module):\n",
    "    def __init__(self):\n",
    "         super(Flatten, self).__init__()\n",
    "    \n",
    "    def updateOutput(self, input):\n",
    "        self.output = input.reshape(len(input), -1)\n",
    "        return self.output\n",
    "    \n",
    "    def updateGradInput(self, input, gradOutput):\n",
    "        self.gradInput = gradOutput.reshape(input.shape)\n",
    "        return self.gradInput\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Flatten\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 0: Initialization (0.01 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "495ee7017854a686dcb711999a798d41",
     "grade": true,
     "grade_id": "cell-3473b7b6ffd64d07",
     "locked": true,
     "points": 0.01,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# do not change this cell\n",
    "import sys\n",
    "\n",
    "import numpy\n",
    "import numpy as np\n",
    "import unittest\n",
    "\n",
    "import collections\n",
    "import pickle\n",
    "import io\n",
    "\n",
    "\n",
    "numpy.variance = numpy.var\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: Linear layer (0.04 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "43c55aeef47a245f9747b49a5e1d0ea0",
     "grade": true,
     "grade_id": "cell-e3503c286039ec55",
     "locked": true,
     "points": 0.04,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "batch_size, n_in, n_out = 2, 3, 4\n",
    "for d in data['test_Linear']:\n",
    "    # layers initialization\n",
    "    custom_layer = Linear(n_in, n_out)\n",
    "    custom_layer.W = d['init_w']\n",
    "    custom_layer.b = d['init_w_b']\n",
    "\n",
    "    layer_input = d['input']\n",
    "    next_layer_grad = d['next_layer_grad']\n",
    "\n",
    "    # 1. check layer output\n",
    "    custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "    gt_output = d['output']\n",
    "    assert np.allclose(gt_output, custom_layer_output, atol=1e-6)\n",
    "\n",
    "    # 2. check layer input grad\n",
    "    custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "    gt_layer_grad = d['grad_output']\n",
    "    assert np.allclose(gt_layer_grad, custom_layer_grad, atol=1e-6)\n",
    "\n",
    "    # 3. check layer parameters grad\n",
    "    custom_layer.accGradParameters(layer_input, next_layer_grad)\n",
    "    weight_grad = custom_layer.gradW\n",
    "    bias_grad = custom_layer.gradb\n",
    "    torch_weight_grad = d['w_grad']\n",
    "    torch_bias_grad = d['b_grad']\n",
    "    assert np.allclose(torch_weight_grad, weight_grad, atol=1e-6)\n",
    "    assert np.allclose(torch_bias_grad, bias_grad, atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: Softmax (0.1 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "853ead287715eea03766c5a047432ee3",
     "grade": true,
     "grade_id": "cell-e2c4124a6f815118",
     "locked": true,
     "points": 0.1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "batch_size, n_in = 2, 4\n",
    "for d in data['test_SoftMax']:\n",
    "    # layers initialization\n",
    "    custom_layer = SoftMax()\n",
    "\n",
    "    layer_input = d['input']\n",
    "    next_layer_grad = d['next_layer_grad']\n",
    "\n",
    "    # 1. check layer output\n",
    "    custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "    gt_output = d['output']\n",
    "    assert np.allclose(gt_output, custom_layer_output, atol=1e-5)\n",
    "\n",
    "    # 2. check layer input grad\n",
    "    custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "    assert np.allclose(d['grad_output'], custom_layer_grad, atol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3: LogSoftMax (0.1 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "913f891bb4fb0e38fb454912eb02f1c6",
     "grade": true,
     "grade_id": "cell-69473387a23d8dff",
     "locked": true,
     "points": 0.1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "batch_size, n_in = 2, 4\n",
    "for d in data['test_LogSoftMax']:\n",
    "    # layers initialization\n",
    "    custom_layer = LogSoftMax()\n",
    "\n",
    "    layer_input = d['input']\n",
    "    next_layer_grad = d['next_layer_grad']\n",
    "\n",
    "    # 1. check layer output\n",
    "    custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "    gt_output = d['output']\n",
    "    np.allclose(gt_output, custom_layer_output, atol=1e-6)\n",
    "\n",
    "    # 2. check layer input grad\n",
    "    custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "    gt_grad = d['grad_output']\n",
    "    np.allclose(gt_grad, custom_layer_grad, atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 4: BatchNormalization (0.1 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "956aa519dabe432152a0ede8a3752a56",
     "grade": true,
     "grade_id": "cell-e5f59af66e6c111b",
     "locked": true,
     "points": 0.1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "batch_size, n_in = 32, 16\n",
    "for d in data['test_BatchNormalization']:\n",
    "    # layers initialization\n",
    "    alpha = 0.9\n",
    "    custom_layer = BatchNormalization(alpha)\n",
    "    custom_layer.train()\n",
    "    init_moving_mean = d['init_moving_mean']\n",
    "    init_moving_variance = d['init_moving_variance']\n",
    "    custom_layer.moving_mean = init_moving_mean\n",
    "    custom_layer.moving_variance = init_moving_variance\n",
    "\n",
    "    layer_input = d['input']\n",
    "    next_layer_grad = d['next_layer_grad']\n",
    "\n",
    "    # 1. check layer output\n",
    "    custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "    gt_output = d['output']\n",
    "    np.allclose(gt_output, custom_layer_output, atol=1e-6)\n",
    "\n",
    "    # 2. check layer input grad\n",
    "    custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "    gt_grad = d['grad_output']\n",
    "    # please, don't increase `atol` parameter, it's garanteed that you can implement batch norm layer\n",
    "    # with tolerance 1e-5\n",
    "    np.allclose(gt_grad, custom_layer_grad, atol=1e-5)\n",
    "\n",
    "    # 3. check moving mean\n",
    "    gt_running_mean = d['gt_moving_mean']\n",
    "    gt_running_var = d['gt_moving_var']\n",
    "    np.allclose(custom_layer.moving_mean, gt_running_mean)\n",
    "    # we don't check moving_variance because pytorch uses slightly different formula for it:\n",
    "    # it computes moving average for unbiased variance (i.e var*N/(N-1))\n",
    "    #self.assertTrue(np.allclose(custom_layer.moving_variance, torch_layer.running_var.numpy()))\n",
    "\n",
    "    # 4. check evaluation mode\n",
    "    custom_layer.moving_variance = gt_running_var\n",
    "    custom_layer.evaluate()\n",
    "    custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "    eval_output = d['eval_output']\n",
    "    np.allclose(eval_output, custom_layer_output, atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 5: Sequential (0.1 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8073cadf9a101e69fc84567477431581",
     "grade": true,
     "grade_id": "cell-1a9c1e3609ed9aab",
     "locked": true,
     "points": 0.1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "batch_size, n_in = 2, 4\n",
    "for d in data['test_Sequential']:\n",
    "    # layers initialization\n",
    "    alpha = 0.9\n",
    "    custom_layer = Sequential()\n",
    "    bn_layer = BatchNormalization(alpha)\n",
    "    bn_layer.moving_mean = d['init_moving_mean']\n",
    "    bn_layer.moving_variance = d['init_moving_var']\n",
    "\n",
    "    custom_layer.add(bn_layer)\n",
    "    scaling_layer = ChannelwiseScaling(n_in)\n",
    "    scaling_layer.gamma = d['init_gamma']\n",
    "    scaling_layer.beta = d['init_beta']\n",
    "    custom_layer.add(scaling_layer)\n",
    "    custom_layer.train()\n",
    "\n",
    "    layer_input = d['input']\n",
    "    next_layer_grad = d['next_layer_grad']\n",
    "\n",
    "    # 1. check layer output\n",
    "    custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "    assert np.allclose(d['gt_output'], custom_layer_output, atol=1e-5)\n",
    "\n",
    "    # 2. check layer input grad\n",
    "    custom_layer_grad = custom_layer.backward(layer_input, next_layer_grad)\n",
    "    assert np.allclose(d['gt_grad'], custom_layer_grad, atol=1e-5)\n",
    "\n",
    "    # 3. check layer parameters grad\n",
    "    weight_grad, bias_grad = custom_layer.getGradParameters()[1]\n",
    "    torch_weight_grad = d['gt_weight_grad']\n",
    "    torch_bias_grad = d['gt_bias_grad']\n",
    "    assert np.allclose(torch_weight_grad, weight_grad, atol=1e-5)\n",
    "    assert np.allclose(torch_bias_grad, bias_grad, atol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 6: Dropout (0.075 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2a5deb8722e44de6c892bc73a80df812",
     "grade": true,
     "grade_id": "cell-1ddb0377b6c68deb",
     "locked": true,
     "points": 0.15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "batch_size, n_in = 2, 4\n",
    "for _ in range(100):\n",
    "    # layers initialization\n",
    "    p = np.random.uniform(0.3, 0.7)\n",
    "    layer = Dropout(p)\n",
    "    layer.train()\n",
    "\n",
    "    layer_input = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "    next_layer_grad = np.random.uniform(-5, 5, (batch_size, n_in)).astype(np.float32)\n",
    "\n",
    "    # 1. check layer output\n",
    "    layer_output = layer.updateOutput(layer_input)\n",
    "    assert np.all(np.logical_or(np.isclose(layer_output, 0),\n",
    "                                np.isclose(layer_output*(1.-p), layer_input)))\n",
    "\n",
    "    # 2. check layer input grad\n",
    "    layer_grad = layer.updateGradInput(layer_input, next_layer_grad)\n",
    "    assert np.all(np.logical_or(np.isclose(layer_grad, 0),\n",
    "                                np.isclose(layer_grad*(1.-p), next_layer_grad)))\n",
    "\n",
    "    # 3. check evaluation mode\n",
    "    layer.evaluate()\n",
    "    layer_output = layer.updateOutput(layer_input)\n",
    "    assert np.allclose(layer_output, layer_input)\n",
    "\n",
    "    # 4. check mask\n",
    "    p = 0.0\n",
    "    layer = Dropout(p)\n",
    "    layer.train()\n",
    "    layer_output = layer.updateOutput(layer_input)\n",
    "    assert np.allclose(layer_output, layer_input)\n",
    "\n",
    "    p = 0.5\n",
    "    layer = Dropout(p)\n",
    "    layer.train()\n",
    "    layer_input = np.random.uniform(5, 10, (batch_size, n_in)).astype(np.float32)\n",
    "    next_layer_grad = np.random.uniform(5, 10, (batch_size, n_in)).astype(np.float32)\n",
    "    layer_output = layer.updateOutput(layer_input)\n",
    "    zeroed_elem_mask = np.isclose(layer_output, 0)\n",
    "    layer_grad = layer.updateGradInput(layer_input, next_layer_grad)\n",
    "    assert np.all(zeroed_elem_mask == np.isclose(layer_grad, 0))\n",
    "\n",
    "    # 5. dropout mask should be generated independently for every input matrix element, not for row/column\n",
    "    batch_size, n_in = 1000, 1\n",
    "    p = 0.8\n",
    "    layer = Dropout(p)\n",
    "    layer.train()\n",
    "\n",
    "    layer_input = np.random.uniform(5, 10, (batch_size, n_in)).astype(np.float32)\n",
    "    layer_output = layer.updateOutput(layer_input)\n",
    "    assert np.sum(np.isclose(layer_output, 0)) != layer_input.size\n",
    "\n",
    "    layer_input = layer_input.T\n",
    "    layer_output = layer.updateOutput(layer_input)\n",
    "    assert np.sum(np.isclose(layer_output, 0)) != layer_input.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 7: LeakyReLU (0.05 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6a6d3bf92fc934b4849cc25a685ee90e",
     "grade": true,
     "grade_id": "cell-da6c6ccf757a0621",
     "locked": true,
     "points": 0.05,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "batch_size, n_in = 2, 4\n",
    "for d in data['test_LeakyReLU']:\n",
    "    # layers initialization\n",
    "    slope = d['slope']\n",
    "    custom_layer = LeakyReLU(slope)\n",
    "\n",
    "    layer_input = d['layer_input']\n",
    "    next_layer_grad = d['next_layer_grad']\n",
    "\n",
    "    # 1. check layer output\n",
    "    custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "    assert np.allclose(d['gt_output'], custom_layer_output, atol=1e-6)\n",
    "\n",
    "    # 2. check layer input grad\n",
    "    custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "    assert np.allclose(d['gt_grad'], custom_layer_grad, atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 8: ELU (0.075 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7fef24e9868d89ff9237dd4dd980d19b",
     "grade": true,
     "grade_id": "cell-301cefbceb12834e",
     "locked": true,
     "points": 0.075,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "batch_size, n_in = 2, 4\n",
    "for d in data['test_ELU']:\n",
    "    # layers initialization\n",
    "    alpha = 1.0\n",
    "    custom_layer = ELU(alpha)\n",
    "\n",
    "    layer_input = d['layer_input']\n",
    "    next_layer_grad = d['next_layer_grad']\n",
    "\n",
    "    # 1. check layer output\n",
    "    custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "    assert np.allclose(d['gt_output'], custom_layer_output, atol=1e-6)\n",
    "\n",
    "    # 2. check layer input grad\n",
    "    custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "    assert np.allclose(d['gt_grad'], custom_layer_grad, atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 9: SoftPlus (0.1 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e9f0e55c87fa6980548152a0a61b0702",
     "grade": true,
     "grade_id": "cell-f804d9bf6f70e562",
     "locked": true,
     "points": 0.1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "batch_size, n_in = 2, 4\n",
    "for d in data['test_SoftPlus']:\n",
    "    # layers initialization\n",
    "    custom_layer = SoftPlus()\n",
    "\n",
    "    layer_input = d['layer_input']\n",
    "    next_layer_grad = d['next_layer_grad']\n",
    "\n",
    "    # 1. check layer output\n",
    "    custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "    assert np.allclose(d['gt_output'], custom_layer_output, atol=1e-6)\n",
    "\n",
    "    # 2. check layer input grad\n",
    "    custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "    assert np.allclose(d['gt_grad'], custom_layer_grad, atol=1e-6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 10: ClassNLLCriterionUnstable (0.1 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae440874a6bb55cb01ffbf1b3954ec8a",
     "grade": true,
     "grade_id": "cell-76b1d3d075e9752b",
     "locked": true,
     "points": 0.1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "batch_size, n_in = 2, 4\n",
    "for d in data['test_ClassNLLCriterionUnstable']:\n",
    "    # layers initialization\n",
    "    custom_layer = ClassNLLCriterionUnstable()\n",
    "\n",
    "    layer_input = d['layer_input']\n",
    "    target = d['target']\n",
    "\n",
    "    # 1. check layer output\n",
    "    custom_layer_output = custom_layer.updateOutput(layer_input, target)\n",
    "    assert np.allclose(d['gt_output'], custom_layer_output, atol=1e-6)\n",
    "\n",
    "    # 2. check layer input grad\n",
    "    custom_layer_grad = custom_layer.updateGradInput(layer_input, target)\n",
    "    assert np.allclose(d['gt_grad'], custom_layer_grad, atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 11: ClassNLLCriterion (0.05 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "de95a17ede1d7eb0260fe998d322df66",
     "grade": true,
     "grade_id": "cell-cbc87cb79435ffda",
     "locked": true,
     "points": 0.05,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "batch_size, n_in = 2, 4\n",
    "for d in data['test_ClassNLLCriterion']:\n",
    "    # layers initialization\n",
    "    custom_layer = ClassNLLCriterion()\n",
    "\n",
    "    layer_input = d['layer_input']\n",
    "    target = d['target']\n",
    "\n",
    "    # 1. check layer output\n",
    "    custom_layer_output = custom_layer.updateOutput(layer_input, target)\n",
    "    assert np.allclose(d['gt_output'], custom_layer_output, atol=1e-6)\n",
    "\n",
    "    # 2. check layer input grad\n",
    "    custom_layer_grad = custom_layer.updateGradInput(layer_input, target)\n",
    "    assert np.allclose(d['gt_grad'], custom_layer_grad, atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 12: Adam (0.1 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "02ae6923f9ab677df706edaa2ace8f26",
     "grade": true,
     "grade_id": "cell-31b7bd19c0cfd8f5",
     "locked": true,
     "points": 0.1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "state = {}\n",
    "config = {'learning_rate': 1e-3, 'beta1': 0.9, 'beta2':0.999, 'epsilon':1e-8}\n",
    "variables = [[np.arange(10).astype(np.float64)]]\n",
    "gradients = [[np.arange(10).astype(np.float64)]]\n",
    "adam_optimizer(variables, gradients, config, state)\n",
    "assert np.allclose(state['m'][0], np.array([0. , 0.1, 0.2, 0.3, 0.4, 0.5,\n",
    "                                                     0.6, 0.7, 0.8, 0.9]))\n",
    "assert np.allclose(state['v'][0], np.array([0., 0.001, 0.004, 0.009, 0.016, 0.025,\n",
    "                                                     0.036, 0.049, 0.064, 0.081]))\n",
    "assert state['t'] == 1\n",
    "assert np.allclose(variables[0][0], np.array([0., 0.999, 1.999, 2.999, 3.999, 4.999,\n",
    "                                                       5.999, 6.999, 7.999, 8.999]))\n",
    "adam_optimizer(variables, gradients, config, state)\n",
    "assert np.allclose(state['m'][0], np.array([0., 0.19, 0.38, 0.57, 0.76, 0.95, 1.14,\n",
    "                                                     1.33, 1.52, 1.71]))\n",
    "assert np.allclose(state['v'][0], np.array([0., 0.001999, 0.007996, 0.017991,\n",
    "                                                     0.031984, 0.049975, 0.071964, 0.097951,\n",
    "                                                     0.127936, 0.161919]))\n",
    "assert state['t'] == 2\n",
    "assert np.allclose(variables[0][0], np.array([0., 0.998, 1.998, 2.998, 3.998, 4.998,\n",
    "                                                       5.998, 6.998, 7.998, 8.998]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 13: Conv2d (0.35 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "698d34a5c5f8b4a29cc1366be93be511",
     "grade": true,
     "grade_id": "cell-b63afa10ee124617",
     "locked": true,
     "points": 0.35,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "batch_size, n_in, n_out = 2, 3, 4\n",
    "h,w = 5,6\n",
    "kern_size = 3\n",
    "for d in data['test_Conv2d']:\n",
    "    # layers initialization\n",
    "    custom_layer = Conv2d(n_in, n_out, kern_size)\n",
    "    custom_layer.W = d['init_w']\n",
    "    custom_layer.b = d['init_b']\n",
    "\n",
    "    layer_input = d['input']\n",
    "    next_layer_grad = d['next_layer_grad']\n",
    "\n",
    "    # 1. check layer output\n",
    "    custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "    assert np.allclose(d['gt_output'], custom_layer_output, atol=1e-6)\n",
    "\n",
    "    # 2. check layer input grad\n",
    "    custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "    assert np.allclose(d['gt_grad'], custom_layer_grad, atol=1e-6)\n",
    "\n",
    "    # 3. check layer parameters grad\n",
    "    custom_layer.accGradParameters(layer_input, next_layer_grad)\n",
    "    weight_grad = custom_layer.gradW\n",
    "    bias_grad = custom_layer.gradb\n",
    "    assert np.allclose(d['gt_weight_grad'], weight_grad, atol=1e-6, )\n",
    "    assert np.allclose(d['gt_bias_grad'], bias_grad, atol=1e-6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 13: MaxPool2d (0.15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dab96dd2b8af6e265d6c635512e7964e",
     "grade": true,
     "grade_id": "cell-1392d1511ad4da77",
     "locked": true,
     "points": 0.15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "batch_size, n_in = 2, 3\n",
    "h,w = 4,6\n",
    "kern_size = 2\n",
    "for d in data['test_MaxPool2d']:\n",
    "    # layers initialization\n",
    "    custom_layer = MaxPool2d(kern_size)\n",
    "\n",
    "    layer_input = d['input']\n",
    "    next_layer_grad = d['next_layer_grad']\n",
    "\n",
    "    # 1. check layer output\n",
    "    custom_layer_output = custom_layer.updateOutput(layer_input)\n",
    "    assert np.allclose(d['gt_output'], custom_layer_output, atol=1e-6)\n",
    "\n",
    "    # 2. check layer input grad\n",
    "    custom_layer_grad = custom_layer.updateGradInput(layer_input, next_layer_grad)\n",
    "    assert np.allclose(d['gt_grad'], custom_layer_grad, atol=1e-6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
